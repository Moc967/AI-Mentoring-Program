{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scikit_Learn.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "w0d_iE883eXP"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanRHowarth/Artificial-Intelligence-Cloud-and-Edge-Implementations/blob/master/Oxford_scikit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyWa9zaxY5oI"
      },
      "source": [
        "# `scikit-learn` Tutorial\n",
        "* This notebook covers different code implements from the scikit learn library, including:\n",
        "  * Rescaling and Standardizing Data \n",
        "  * Model Evaluation and Metrics \n",
        "  * Classification and regression machine learning algorithms \n",
        "  * Regularization\n",
        "  * Optimising parameters \n",
        "  * Ensemble models \n",
        "  * Building Pipelines\n",
        "* We will use classification and regression datasets from `scikit-learn`, and will take code from the *ands-On Machine Learning with Scikit-Learn and TensorFlow* coursebook, as well as the [Machine Learning with Python Cookbook](https://www.amazon.co.uk/Machine-Learning-Python-Cookbook-Chris/dp/1491989386/ref=pd_sim_14_5?_encoding=UTF8&pd_rd_i=1491989386&pd_rd_r=e03d308c-dd3d-11e8-9244-bbcc5e42676d&pd_rd_w=lQiym&pd_rd_wg=lVZwd&pf_rd_i=desktop-dp-sims&pf_rd_m=A3P5ROKL5A1OLE&pf_rd_p=1e3b4162-429b-4ea8-80b8-75d978d3d89e&pf_rd_r=KBZG3157A75PS5MPGY8K&pf_rd_s=desktop-dp-sims&pf_rd_t=40701&psc=1&refRID=KBZG3157A75PS5MPGY8K).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Approach and Exercises\n",
        "* Note that this notebook is not focussed on how to structure an end to end machine learning problem, but is about extending the code implementations you will see in the end-to-end tutorials to further your knowledge.\n",
        "* This notebook is a progression from the previous notebooks, meaning that the exercises will generally be more involved. We will provide code implementations for one of the datasets and leave space for you to code implementations on a different dataset. Support is available via Teams. \n",
        "* Each section will have an exercise to help reinforce your learning. We suggest you:\n",
        "   * Write out each line of code by hand (rather than copy and paste it from the relevant example) - this will improve your understanding of code syntax\n",
        "   * Write out, above each line of code, an explanation as to what the code, using a comment `#` - this will improve your understanding of how the code works\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA2bg4ruhi0G"
      },
      "source": [
        "## 1. Tutorial set up \n",
        "* Import statements\n",
        "* Loading the datasets\n",
        "* Reviewing the ML problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VrrYp-SY2Z4"
      },
      "source": [
        "# import statements \n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.datasets import load_boston\n",
        " \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        " \n",
        "# Hide all warning messages\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        " \n",
        "# we will generally import scikit learn libraries when they are required, so you understand which specifc libraries are required"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A19qwVSK8oRL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "060c611f-f1d0-4d29-a6c4-de3c1fc8c050"
      },
      "source": [
        "# load the classification dataset\n",
        "class_data = load_breast_cancer()\n",
        "class_X = pd.DataFrame(class_data.data, columns = class_data.feature_names)\n",
        "class_y = class_data.target\n",
        " \n",
        "# check the data has loaded successfully \n",
        "print(class_X.shape)\n",
        "print(class_y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(569, 30)\n",
            "(569,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK67KgsUDAvk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32575405-2772-4973-a0c3-e16d651c828a"
      },
      "source": [
        "# load the regression dataset\n",
        "boston = load_boston()\n",
        "boston_X = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
        "boston_y = boston.target\n",
        " \n",
        "# check the data has loaded successfully \n",
        "print(boston_X.shape)\n",
        "print(boston_y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(506, 13)\n",
            "(506,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em1bucYpgKsS"
      },
      "source": [
        "## 2. Rescaling and Standardizing Data \n",
        "* Machine Learning algorithms can receive performance benefits from data that has been rescaled and standardized, either in terms of speed or accuracy, or both.\n",
        "* There are a number of different ways of doing this, and implementations may vary because of algorithm requirements or personal preference.\n",
        "* We cover two common implementations below, and an additional one in the exercise. We'd encourage further research to understand the maths in more detail."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2dB0Y6qlAq5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efdf6eca-724d-4916-e17b-627c9b7cd3a9"
      },
      "source": [
        "## rescaling using MinMaxScaler, which converts data to a specified range, usually (0,1) or (-1,1)\n",
        "# many machine learning algorithms assume data is on the same scale  \n",
        " \n",
        "# here we import the necessary library \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        " \n",
        "# Create feature - we will use one feature but you may just pass all the features to rescaler \n",
        "feature = class_X['texture error']\n",
        " \n",
        "# print feature prior to rescaling\n",
        "print(\"Features prior to rescaling:\\n\")\n",
        "print(feature[0:10])\n",
        " \n",
        "# we will print the shape out as well \n",
        "print(\"\\nFeature shape prior to reshaping:\")\n",
        "print(feature.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features prior to rescaling:\n",
            "\n",
            "0    0.9053\n",
            "1    0.7339\n",
            "2    0.7869\n",
            "3    1.1560\n",
            "4    0.7813\n",
            "5    0.8902\n",
            "6    0.7732\n",
            "7    1.3770\n",
            "8    1.0020\n",
            "9    1.5990\n",
            "Name: texture error, dtype: float64\n",
            "\n",
            "Feature shape prior to reshaping:\n",
            "(569,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Puz03eB8SCDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ea6123-ceaf-42ab-8fe5-6b722ee3eaa2"
      },
      "source": [
        "## because we have used a pandas column, our data has the wrong shape for the function\n",
        "# we need a 2D tuple\n",
        " \n",
        "# this will reshape the data \n",
        "feature = feature.values.reshape(-1,1)\n",
        " \n",
        "# notice we now have a (samples,feature) tuple not just a (samples,) tuple\n",
        "print(\"\\nFeature shape after reshaping:\")\n",
        "feature.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Feature shape after reshaping:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3VSoI24RvCf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a50d266e-4629-4dfc-b188-b0f7ce8daf03"
      },
      "source": [
        "# we need to instantiate the scaler and pass in the range we want our values to be scaled within \n",
        "minmax_scale = MinMaxScaler(feature_range=(0, 1))\n",
        " \n",
        "# Scale the feature - note the fit_transform function, in the next example we will separate these steps\n",
        "scaled_feature = minmax_scale.fit_transform(feature)\n",
        " \n",
        "# lets look at the features now we have rescaled\n",
        "print(\"Features after rescaling:\\n\")\n",
        "scaled_feature[0:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features after rescaling:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.12046941],\n",
              "       [0.08258929],\n",
              "       [0.09430251],\n",
              "       [0.17587518],\n",
              "       [0.09306489],\n",
              "       [0.11713225],\n",
              "       [0.09127475],\n",
              "       [0.22471711],\n",
              "       [0.14184052],\n",
              "       [0.27378006]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcILmN5JlA5W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "0573bce9-585d-4392-ba8a-ab016d366939"
      },
      "source": [
        "## standardizing rescales features so that they are normally distributed, with a mean of 0 and a std deviation of 1\n",
        " \n",
        "# import the appropriate library\n",
        "from sklearn.preprocessing import StandardScaler\n",
        " \n",
        "# we will transform all the features in the dataset, so let's look at it  prior to transformation\n",
        "mean_std = pd.DataFrame(data={'mean':class_X.mean(), 'std':class_X.std()})\n",
        " \n",
        "# call the dataframe\n",
        "mean_std[0:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>mean radius</th>\n",
              "      <td>14.127292</td>\n",
              "      <td>3.524049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean texture</th>\n",
              "      <td>19.289649</td>\n",
              "      <td>4.301036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean perimeter</th>\n",
              "      <td>91.969033</td>\n",
              "      <td>24.298981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean area</th>\n",
              "      <td>654.889104</td>\n",
              "      <td>351.914129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean smoothness</th>\n",
              "      <td>0.096360</td>\n",
              "      <td>0.014064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean compactness</th>\n",
              "      <td>0.104341</td>\n",
              "      <td>0.052813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean concavity</th>\n",
              "      <td>0.088799</td>\n",
              "      <td>0.079720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean concave points</th>\n",
              "      <td>0.048919</td>\n",
              "      <td>0.038803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean symmetry</th>\n",
              "      <td>0.181162</td>\n",
              "      <td>0.027414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <td>0.062798</td>\n",
              "      <td>0.007060</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              mean         std\n",
              "mean radius              14.127292    3.524049\n",
              "mean texture             19.289649    4.301036\n",
              "mean perimeter           91.969033   24.298981\n",
              "mean area               654.889104  351.914129\n",
              "mean smoothness           0.096360    0.014064\n",
              "mean compactness          0.104341    0.052813\n",
              "mean concavity            0.088799    0.079720\n",
              "mean concave points       0.048919    0.038803\n",
              "mean symmetry             0.181162    0.027414\n",
              "mean fractal dimension    0.062798    0.007060"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khSirtDXVRAs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62717aa5-0cc9-4e3c-e42c-82a68214df2c"
      },
      "source": [
        "# Create scaler\n",
        "scaler = StandardScaler()\n",
        " \n",
        "# fit the scaler - this calculates the minimum and maximum values of the data\n",
        "scaler.fit(class_X)\n",
        " \n",
        "# transform the data using the fitted scaler - this applies the transform using the fit\n",
        "standardized = scaler.transform(class_X)\n",
        " \n",
        "# this shows the features transformed - note that this returns an array not a dataframe\n",
        "standardized[0:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.09706398e+00, -2.07333501e+00,  1.26993369e+00,\n",
              "         9.84374905e-01,  1.56846633e+00,  3.28351467e+00,\n",
              "         2.65287398e+00,  2.53247522e+00,  2.21751501e+00,\n",
              "         2.25574689e+00,  2.48973393e+00, -5.65265059e-01,\n",
              "         2.83303087e+00,  2.48757756e+00, -2.14001647e-01,\n",
              "         1.31686157e+00,  7.24026158e-01,  6.60819941e-01,\n",
              "         1.14875667e+00,  9.07083081e-01,  1.88668963e+00,\n",
              "        -1.35929347e+00,  2.30360062e+00,  2.00123749e+00,\n",
              "         1.30768627e+00,  2.61666502e+00,  2.10952635e+00,\n",
              "         2.29607613e+00,  2.75062224e+00,  1.93701461e+00],\n",
              "       [ 1.82982061e+00, -3.53632408e-01,  1.68595471e+00,\n",
              "         1.90870825e+00, -8.26962447e-01, -4.87071673e-01,\n",
              "        -2.38458552e-02,  5.48144156e-01,  1.39236330e-03,\n",
              "        -8.68652457e-01,  4.99254601e-01, -8.76243603e-01,\n",
              "         2.63326966e-01,  7.42401948e-01, -6.05350847e-01,\n",
              "        -6.92926270e-01, -4.40780058e-01,  2.60162067e-01,\n",
              "        -8.05450380e-01, -9.94437403e-02,  1.80592744e+00,\n",
              "        -3.69203222e-01,  1.53512599e+00,  1.89048899e+00,\n",
              "        -3.75611957e-01, -4.30444219e-01, -1.46748968e-01,\n",
              "         1.08708430e+00, -2.43889668e-01,  2.81189987e-01],\n",
              "       [ 1.57988811e+00,  4.56186952e-01,  1.56650313e+00,\n",
              "         1.55888363e+00,  9.42210440e-01,  1.05292554e+00,\n",
              "         1.36347845e+00,  2.03723076e+00,  9.39684817e-01,\n",
              "        -3.98007910e-01,  1.22867595e+00, -7.80083377e-01,\n",
              "         8.50928301e-01,  1.18133606e+00, -2.97005012e-01,\n",
              "         8.14973504e-01,  2.13076435e-01,  1.42482747e+00,\n",
              "         2.37035535e-01,  2.93559404e-01,  1.51187025e+00,\n",
              "        -2.39743838e-02,  1.34747521e+00,  1.45628455e+00,\n",
              "         5.27407405e-01,  1.08293217e+00,  8.54973944e-01,\n",
              "         1.95500035e+00,  1.15225500e+00,  2.01391209e-01],\n",
              "       [-7.68909287e-01,  2.53732112e-01, -5.92687167e-01,\n",
              "        -7.64463792e-01,  3.28355348e+00,  3.40290899e+00,\n",
              "         1.91589718e+00,  1.45170736e+00,  2.86738293e+00,\n",
              "         4.91091929e+00,  3.26373441e-01, -1.10409044e-01,\n",
              "         2.86593405e-01, -2.88378148e-01,  6.89701660e-01,\n",
              "         2.74428041e+00,  8.19518384e-01,  1.11500701e+00,\n",
              "         4.73268037e+00,  2.04751088e+00, -2.81464464e-01,\n",
              "         1.33984094e-01, -2.49939304e-01, -5.50021228e-01,\n",
              "         3.39427470e+00,  3.89339743e+00,  1.98958826e+00,\n",
              "         2.17578601e+00,  6.04604135e+00,  4.93501034e+00],\n",
              "       [ 1.75029663e+00, -1.15181643e+00,  1.77657315e+00,\n",
              "         1.82622928e+00,  2.80371830e-01,  5.39340452e-01,\n",
              "         1.37101143e+00,  1.42849277e+00, -9.56046689e-03,\n",
              "        -5.62449981e-01,  1.27054278e+00, -7.90243702e-01,\n",
              "         1.27318941e+00,  1.19035676e+00,  1.48306716e+00,\n",
              "        -4.85198799e-02,  8.28470780e-01,  1.14420474e+00,\n",
              "        -3.61092272e-01,  4.99328134e-01,  1.29857524e+00,\n",
              "        -1.46677038e+00,  1.33853946e+00,  1.22072425e+00,\n",
              "         2.20556166e-01, -3.13394511e-01,  6.13178758e-01,\n",
              "         7.29259257e-01, -8.68352984e-01, -3.97099619e-01],\n",
              "       [-4.76374665e-01, -8.35335303e-01, -3.87148067e-01,\n",
              "        -5.05650454e-01,  2.23742148e+00,  1.24433549e+00,\n",
              "         8.66301596e-01,  8.24655646e-01,  1.00540180e+00,\n",
              "         1.89000504e+00, -2.55070294e-01, -5.92661652e-01,\n",
              "        -3.21304185e-01, -2.89258217e-01,  1.56346702e-01,\n",
              "         4.45543649e-01,  1.60025198e-01, -6.91235537e-02,\n",
              "         1.34118807e-01,  4.86845840e-01, -1.65498247e-01,\n",
              "        -3.13836333e-01, -1.15009456e-01, -2.44320208e-01,\n",
              "         2.04851283e+00,  1.72161644e+00,  1.26324320e+00,\n",
              "         9.05887786e-01,  1.75406939e+00,  2.24180161e+00],\n",
              "       [ 1.17090767e+00,  1.60649427e-01,  1.13812505e+00,\n",
              "         1.09529491e+00, -1.23136226e-01,  8.82952423e-02,\n",
              "         3.00072399e-01,  6.46935108e-01, -6.43246179e-02,\n",
              "        -7.62332153e-01,  1.49883071e-01, -8.04939888e-01,\n",
              "         1.55410293e-01,  2.98627465e-01, -9.09029826e-01,\n",
              "        -6.51568010e-01, -3.10141387e-01, -2.28089026e-01,\n",
              "        -8.29666081e-01, -6.11217806e-01,  1.36898330e+00,\n",
              "         3.22882892e-01,  1.36832530e+00,  1.27521954e+00,\n",
              "         5.18640227e-01,  2.12149800e-02,  5.09552250e-01,\n",
              "         1.19671580e+00,  2.62475664e-01, -1.47304787e-02],\n",
              "       [-1.18516778e-01,  3.58450132e-01, -7.28668396e-02,\n",
              "        -2.18964911e-01,  1.60404905e+00,  1.14010235e+00,\n",
              "         6.10257495e-02,  2.81950258e-01,  1.40335463e+00,\n",
              "         1.66035318e+00,  6.43623001e-01,  2.90560957e-01,\n",
              "         4.90050986e-01,  2.33722421e-01,  5.88030871e-01,\n",
              "         2.68932704e-01, -2.32553954e-01,  4.35348506e-01,\n",
              "        -6.88004232e-01,  6.11668783e-01,  1.63762976e-01,\n",
              "         4.01047912e-01,  9.94485804e-02,  2.88594274e-02,\n",
              "         1.44796112e+00,  7.24785507e-01, -2.10538519e-02,\n",
              "         6.24195735e-01,  4.77640485e-01,  1.72643451e+00],\n",
              "       [-3.20166857e-01,  5.88829778e-01, -1.84080380e-01,\n",
              "        -3.84207273e-01,  2.20183876e+00,  1.68400981e+00,\n",
              "         1.21909628e+00,  1.15069158e+00,  1.96559991e+00,\n",
              "         1.57246173e+00, -3.56850016e-01, -3.89818004e-01,\n",
              "        -2.27743400e-01, -3.52403123e-01, -4.36677342e-01,\n",
              "         5.33290226e-01,  1.20568341e-01,  7.52430487e-02,\n",
              "         1.07481537e-01, -1.73631991e-02, -1.61356597e-01,\n",
              "         8.22813332e-01, -3.16091086e-02, -2.48363407e-01,\n",
              "         1.66275699e+00,  1.81830968e+00,  1.28003453e+00,\n",
              "         1.39161624e+00,  2.38985717e+00,  1.28864955e+00],\n",
              "       [-4.73534523e-01,  1.10543868e+00, -3.29481787e-01,\n",
              "        -5.09063378e-01,  1.58269942e+00,  2.56335845e+00,\n",
              "         1.73887209e+00,  9.41760326e-01,  7.97298024e-01,\n",
              "         2.78309559e+00, -3.88250143e-01,  6.93345302e-01,\n",
              "        -4.09419634e-01, -3.60763773e-01,  3.60084898e-02,\n",
              "         2.60958662e+00,  1.50984760e+00,  4.09394960e-01,\n",
              "        -3.21136366e-01,  2.37734605e+00, -2.44189609e-01,\n",
              "         2.44310906e+00, -2.86278027e-01, -2.97409172e-01,\n",
              "         2.32029536e+00,  5.11287727e+00,  3.99543285e+00,\n",
              "         1.62001520e+00,  2.37044380e+00,  6.84685604e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIRzFhg0X_cA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "7b8ca8ca-450d-44b4-f524-9fd1d33b342c"
      },
      "source": [
        "# lets have another look at our transformed data\n",
        "mean_std_transformed = pd.DataFrame(data={'mean':standardized.mean(axis=0), 'std':standardized.std(axis=0)}, index=class_X.columns)\n",
        " \n",
        "# call the dataframe\n",
        "mean_std_transformed[0:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>mean radius</th>\n",
              "      <td>-3.162867e-15</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean texture</th>\n",
              "      <td>-6.530609e-15</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean perimeter</th>\n",
              "      <td>-7.078891e-16</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean area</th>\n",
              "      <td>-8.799835e-16</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean smoothness</th>\n",
              "      <td>6.132177e-15</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean compactness</th>\n",
              "      <td>-1.120369e-15</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean concavity</th>\n",
              "      <td>-4.421380e-16</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean concave points</th>\n",
              "      <td>9.732500e-16</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean symmetry</th>\n",
              "      <td>-1.971670e-15</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <td>-1.453631e-15</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                mean  std\n",
              "mean radius            -3.162867e-15  1.0\n",
              "mean texture           -6.530609e-15  1.0\n",
              "mean perimeter         -7.078891e-16  1.0\n",
              "mean area              -8.799835e-16  1.0\n",
              "mean smoothness         6.132177e-15  1.0\n",
              "mean compactness       -1.120369e-15  1.0\n",
              "mean concavity         -4.421380e-16  1.0\n",
              "mean concave points     9.732500e-16  1.0\n",
              "mean symmetry          -1.971670e-15  1.0\n",
              "mean fractal dimension -1.453631e-15  1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS2xLaapj-xV"
      },
      "source": [
        "### EXERCISE SECTION 2: \n",
        "* Apply the `Rescaling` and `Standardization` functions to one or more features of the `Boston` dataset\n",
        "* Research and implement a `Normalizer` function\n",
        "* Have a think about what circumstances each of these functions might be most appropriate to apply to different data and why \n",
        "* We can discuss the implementations and benefits of each approach on Teams "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK0B4ANXmQiE"
      },
      "source": [
        "## EXERCISE CODE GOES HERE \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8E5I4vtgK7y"
      },
      "source": [
        "## 3. Model Evaluation and Metrics \n",
        "* We need to understand how we are going to assess our model's performance. This involves:\n",
        "  * Splitting our Training and Testing data appropriately \n",
        "  * Choosing an Evaluation Metric\n",
        "  * Developing a Baseline Model \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq-9z9Hgnwia"
      },
      "source": [
        "## testing and training splits \n",
        " \n",
        "# import statement \n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "# this splits our data into a test and training set\n",
        "# the function shuffles our data before splitting (so we will get both classes in both sets)\n",
        "# setting a random_state means that this shuffling will be consistent for each run \n",
        "# setting stratify=class_y tells the function to have an even number of class labels in each set\n",
        "X_train, X_test, y_train, y_test = train_test_split(class_X, class_y, test_size=0.3, random_state=1, stratify = class_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LcIe8dY_50z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b329072b-7f0f-4c60-a21b-7d5410282b35"
      },
      "source": [
        "# we can verify the stratifications using np.bincount\n",
        "print('Labels counts in y:', np.bincount(class_y))\n",
        "print('Percentage of class zeroes in class_y',np.round(np.bincount(class_y)[0]/len(class_y)*100))\n",
        " \n",
        "print(\"\\n\")\n",
        "print('Labels counts in y_train:', np.bincount(y_train))\n",
        "print('Percentage of class zeroes in y_train',np.round(np.bincount(y_train)[0]/len(y_train)*100))\n",
        " \n",
        "print(\"\\n\")\n",
        "print('Labels counts in y_test:', np.bincount(y_test))\n",
        "print('Percentage of class zeroes in y_test',np.round(np.bincount(y_test)[0]/len(y_test)*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labels counts in y: [212 357]\n",
            "Percentage of class zeroes in class_y 37.0\n",
            "\n",
            "\n",
            "Labels counts in y_train: [148 250]\n",
            "Percentage of class zeroes in y_train 37.0\n",
            "\n",
            "\n",
            "Labels counts in y_test: [ 64 107]\n",
            "Percentage of class zeroes in y_test 37.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEM_Ub4wCx7I"
      },
      "source": [
        "* Note that there are additional ways to divide our data into test and training sets, and we cover `K-fold cross validation` in the **Pipeline** section below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLckno3Rnw-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4b2f85b-a866-473d-b291-1bd8af54c4a0"
      },
      "source": [
        "## we can create a baseline model to benchmark our other estimators against\n",
        "## this can be a simple estimator or we can use a dummy estimator to make predictions in a random manner \n",
        " \n",
        "# this is the required import statement \n",
        "from sklearn.dummy import DummyClassifier\n",
        " \n",
        "# this creates our dummy classifier, and the value we pass in to the strategy parameter determines how the \n",
        "# classifier will make the predictions\n",
        "dummy = DummyClassifier(strategy='uniform', random_state=1)\n",
        " \n",
        "# \"Train\" model\n",
        "dummy.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DummyClassifier(constant=None, random_state=1, strategy='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrBocYv4nwwW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed230c01-12dd-434b-deb7-f5187b3341ba"
      },
      "source": [
        "## evaluating the model metrics\n",
        " \n",
        "# Here we get an accuracy score\n",
        "dummy.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.47953216374269003"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5vlaKNKRPeC"
      },
      "source": [
        "* There are a number of different ways in `scikit-learn` to get an estimator score and it can get confusing first.\n",
        "* Remember that to get a score, we need to instantiate a model, fit it to the data, predict using unseen data, compare the predictions against actual data, and score the difference. This is true for classification and regression problems, and is true no matter the method used to get there.\n",
        "  * So, in the end-to-end tutorials we split the training and test data,  fitted our data to an estimator, and called the `.predict` method on the estimator to get our predictions, and then passed this to a scoring function (four steps)\n",
        "  * In using the `estimator.score()`method above, we are passing in our split data and the method is then making predictions and returning the score (three steps). \n",
        "  * And, in the `cross_val_score()` method used below we are effectively using one step as the method takes an estimator and our data and returns a score. You can find out more about this method [here](https://scikit-learn.org/stable/modules/cross_validation.html) \n",
        "  * Another function cross validation function `cross_validate()` is available\n",
        "  and enables the user to specify multiple scoring metrics at once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEd1YkKQHHqo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "331a7a49-3e45-4b22-95a2-a2088c89dc54"
      },
      "source": [
        "## here we fit a new estimator and use cross_val_score to get a score based on a defined metric \n",
        " \n",
        "# import statements\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        " \n",
        "# instantiate logistic regression classifier\n",
        "logistic = LogisticRegression(solver='liblinear')\n",
        " \n",
        "# we pass our estimator and data to the method. we also specify the number of folds (default is 3)\n",
        "# the default scring method is the one associated with the estimator we pass in\n",
        "# we can use the scoring parameter to pass in different scoring methods. Here we use recall.  \n",
        "cross_val_score(logistic, class_X, class_y, cv=5, scoring=\"recall\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.98591549, 0.97183099, 0.98611111, 0.95833333, 0.95774648])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od2_2Tq1n0pc"
      },
      "source": [
        "### EXERCISE SECTION 3:\n",
        "* Part 1: Implement binary classification scoring functions \n",
        "  * Use `cross_val_score` to implement `f1`, and `precision` scores \n",
        "  * Understand what these scores, and the `accuracy` and `recall` scores implemented above, tell us\n",
        "* Part 2: Another way of assessing our model's performance is with the `Receiving Operating Characteristic (ROC) Curve`. \n",
        "  * What does this show us? Can you implement it?\n",
        "  * Like the other questions here, we can work through these via Teams.\n",
        "* Part 3: What are the main ways of evaluating a mutliclass classification problem?\n",
        "  * Research and make notes below\n",
        "* Part 4: Apply the above approach to the Boston dataset. You should think about:\n",
        "  * What sort of test and training split you want to implement \n",
        "    * Be careful to change the variable names for `X_train`, `y_train` etc. so that the new dataset doesn't overwrite those variables\n",
        "    * Experiment with not using the `stratify =  ` variable\n",
        "  * The evaluation metrics you wish to use and why (this is different than above as the Boston dataset is a regression problem)\n",
        "    * Evaluation metrics are covered in *Python Machine Learning*, Chapter 6\n",
        "  * Implementing a baseline model \n",
        "    * *Python Machine Learning*, Chapter 10 has linear regression implementations to draw from.\n",
        "* Feel free to discuss your approach on Teams and ask for support "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89mkhQL_HH3A"
      },
      "source": [
        "## EXERCISE SECTION 3: PART 1\n",
        "## use the cross_val_score function to pass in and return the scoring functions: precision, f1 \n",
        "## note the scores and research what each score represents \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjBM1RwPLW23"
      },
      "source": [
        "## EXERCISE SECTION 3: PART 2\n",
        "## look up and implement the ROC metric \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0vDRITZL1M1"
      },
      "source": [
        "**EXERCISE SECTION 3: PART 3**\n",
        "* What are the evaluation metrics that can be used for mutliclass regression problems?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Answer**\n",
        "* Text here\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wg9cj_52o7Un"
      },
      "source": [
        "## EXERCISE SECTION 3: PART 4 \n",
        "## Implement a baseline model and evaluation metric for the Boston dataset (regression)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptGBIAY4gLI9"
      },
      "source": [
        "## 4. Classification and regression machine learning algorithms "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgIyAS2_qjSV"
      },
      "source": [
        "### EXERCISE SECTION 4:\n",
        "* You already have an idea about how to fit and instatiate machine learning models.\n",
        "* There are a large number of machine learning approaches you can use, and this exercise is about understanding how a number of them perform on our datasets.\n",
        "* Research and instantiate as many algorithms as you wish in the code cell marked below. \n",
        "  * There are a number of examples in *Python Machine Learning*, for example `SVM`, `Decision Trees` and `Logistic Regression` in Chapter 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsEysh7vshDG"
      },
      "source": [
        "## EXERCISE CODE HERE \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIFg9b-7wXgX"
      },
      "source": [
        "## 5. Regularization \n",
        "* When we train our models, we might need to prevent against **over-** or **underfitting**. Overfitting is when our model performs well on the training data but does not generalise to unseen data (such as our test set). The model might be too complex, with the parameter weights probably too large and aligned too closely to the patterns of the training data. The solution is to develop a means of reducing these weights during training, and this is done through a process called **regularization**.\n",
        "* Underfitting is when our model is too simple to capture patterns in either the training or test sets. Chapters 3 and 10 of *Python Machine Learning* cover this topic in more detail.\n",
        "* How we regularise the model depends on what estimator we are using. In most cases, we can penalise the increase in parameter weights as the model trains by inserting a regularization term to our model when we instantiate it. This is applicable to regression models, logistic regression and support vector classifiers (see below for implementations).  \n",
        "* For these estimators, the regularization term is commonly either **L1 Regularization**, where the weights are reduced by a user defined parameter multiplied by the absolute value of the weights (or coefficients); or **L2 Regularization**, where the weights are reduced by a user defined parameter multiplied by the squared sum of the weights.\n",
        "* For tree based models, such as Decision Trees and Random Forests, we can help prevent overfitting by adjusting parameters associated with the leaf and branch settings.\n",
        "* Regularization is another reason why feature scaling such as standardization is important. For regularization to work properly, we need to ensure that all our features are on comparable scales.\n",
        "* We implement L1 and L2 Regularization for regression models in this section. In Section 6, we implement  `RandomSearchCV` (explained below) on a `Logistic Regression` estimator. The parameters that we change in that implementation are related to regularization ('`C`' and '`penalty`')."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuSLgswxwX4w"
      },
      "source": [
        "## For regularization in regression we instantiate new models\n",
        "## Lasso regression is used for L1 Regularization \n",
        " \n",
        "# import statement \n",
        "from sklearn.linear_model import Lasso\n",
        " \n",
        "# Create lasso regression with alpha value - this is our hyperparameter \n",
        "regression = Lasso(alpha=0.5)\n",
        " \n",
        "# Fit the linear regression\n",
        "model_l1 = regression.fit(class_X, class_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LASLHhqSxvPt"
      },
      "source": [
        "## Ridge regression is used for L2 Regularization\n",
        " \n",
        "# import statement \n",
        "from sklearn.linear_model import Ridge\n",
        " \n",
        "# Create ridge regression with an alpha value (our hyperparameter)\n",
        "regression = Ridge(alpha=0.5)\n",
        " \n",
        "# Fit the linear regression\n",
        "model_l2 = regression.fit(class_X, class_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kWobSEgwqad"
      },
      "source": [
        "### EXERCISE FOR SECTION 5\n",
        "* Part 1: Train both the models above and get a score. Compare the results and see if one type of regularization works better than another.\n",
        "* Part 2: The `ElasticNet`estimator combines both L1 and L2 Regularization. Look up the estimator on the scikit-learn documentation and a) write out how it combines both regularization methods, and b) see if you can implement it.\n",
        "* Part 3: We can use `RidgeCV` and `LassoCV` to explore the best regularization parameters for each estimator. Implement one or both of these estimators, defining the parameters you want to search over.\n",
        "* Part 4: Look at the `DecisionTreeClassifier `documentation in `scikit-learn` and come to a view on which parameters can be set to help prevent overfitting (and therefore help regularization). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO1SGtIRwYHA"
      },
      "source": [
        "## EXERCISE SECTION 5: PART 1 \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZAm2HeawYhd"
      },
      "source": [
        "## EXERCISE SECTION 5: PART 2 \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "718i3izO3dEv"
      },
      "source": [
        "## EXERCISE SECTION 5: PART 3\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0d_iE883eXP"
      },
      "source": [
        "#### EXERCISE SECTION 5: PART 4\n",
        "* Answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrVZ5owOgLlh"
      },
      "source": [
        " ## 6. Optimizing parameters \n",
        " * We applied `GridSearchCV` to identify the best hyperparameters for our models in the in-class regression tutorials.\n",
        " * There are other methods available to use that don't take the brute force approach of `GridSearchCV`.\n",
        " * We will cover an implementation of `RandomizedSearchCV` below, and use the exercise for you to implement it on the other datatset.\n",
        "  * We use this method to search over defined hyperparameters, like `GridSearchCV`, however a fixed number of parameters are sampled, as defined by `n_iter` parameter.  \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od1RjPl504pw"
      },
      "source": [
        "# import libraries - note we use scipy for generating a uniform distribution \n",
        "from scipy.stats import uniform\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        " \n",
        "# Create logistic regression\n",
        "logistic = LogisticRegression(solver='liblinear')\n",
        " \n",
        "# we can create hyperparameters as a list, as in a type regularisation penalty \n",
        "penalty = ['l1', 'l2']\n",
        " \n",
        "# or as a distribution of values to sample from -'C' is the hyperparameter controlling the size of the regularisation penalty \n",
        "C = uniform(loc=0, scale=4)\n",
        " \n",
        "# we need to pass these parameters as a dictionary of {param_name: values}\n",
        "hyperparameters = dict(C=C, penalty=penalty)\n",
        " \n",
        "# we instantiate our model\n",
        "randomizedsearch = RandomizedSearchCV(\n",
        "    logistic, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0,\n",
        "    n_jobs=-1)\n",
        " \n",
        "# and fit it to the data \n",
        "best_model = randomizedsearch.fit(class_X, class_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z58WbRA-tDuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7883408d-d60f-4255-f0ff-92f1cb215dc9"
      },
      "source": [
        "# and we can call this method to return the best parameters the search returned\n",
        "best_model.best_estimator_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=3.730229437354635, class_weight=None, dual=False,\n",
              "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
              "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2odbxKo040A"
      },
      "source": [
        "### EXERCISE FOR SECTION 6: \n",
        "* Part 1: Once we have fit the model to our data, we can call different methods on it, for example we can find out what the best parameters were, and we can predict using the best estimator returned by the search. Use the docs [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) to call these methods on the `best_model` variable above. \n",
        "* Part 2: Implement `RandomSearchCV` on the Boston dataset\n",
        "  * Think about which parameters you want to specify\n",
        "  * You can pass these parameters to variables prior to creating the random search (as we do above)\n",
        "  * Then call methods as we do for Part 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms6grwmT048R"
      },
      "source": [
        "## EXERCISE SECTION 6: PART 1 \n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emxT2zlrrzQ5"
      },
      "source": [
        "## EXERCISE SECTION 6: PART 2\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcFWj3HEgL9t"
      },
      "source": [
        "## 7. Ensemble models \n",
        "\n",
        "* The goal of ensemble methods is to combine different classifiers into a meta-classifier that has better generalization performance than each individual classifier alone. \n",
        "* There are several different approaches to achieve this, including **majority voting** ensemble methods, with which we select the class label that has been predicted by the majority of classifiers.\n",
        "* The ensemble can be built from different classification algorithms, such as decision trees, support vector machines, logistic regression classifiers, and so on. Alternatively, we can also use the same base classification algorithm, fitting different subsets of the training set. \n",
        "* Indeed, majority voting will work best if the classifiers used are different from each other and/or trained on different datasets (or subsets of the same data) in order for their errors to be uncorrelated.  Chapter 7 of *Python Machine Learning* has a good discussion of the principles behind Ensemble models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zCHjuFr1orF"
      },
      "source": [
        "# import our classifiers\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        " \n",
        "# lets instantiate individual models \n",
        "log_clf = LogisticRegression(solver='liblinear')\n",
        "rnd_clf = RandomForestClassifier()\n",
        "svm_clf = SVC()\n",
        " \n",
        "# and an ensemble of them\n",
        "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "                              # here we select hard voting, which returns the majority of the predictions, not an average of probabilities\n",
        "                              voting='hard')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blYK41CIoM-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50900345-e1a8-431e-809f-40b6f6fc8c6d"
      },
      "source": [
        "# here we can cycle through the individual estimators \n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "    \n",
        "    # fit them to the training data \n",
        "    clf.fit(X_train, y_train)\n",
        "    \n",
        "    # get a prediction\n",
        "    y_pred = clf.predict(X_test)\n",
        "    \n",
        "    # and print the prediction \n",
        "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression 0.9473684210526315\n",
            "RandomForestClassifier 0.9532163742690059\n",
            "SVC 0.9239766081871345\n",
            "VotingClassifier 0.9532163742690059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIoSkRC31o36"
      },
      "source": [
        "### EXERCISE FOR SECTION 7: \n",
        "* Part 1: Implement **soft voting**.\n",
        " * If all the classifiers used in the ensemble are able to estimate class probabilities (they have a `predict_proba()` method that can be called on the estimator), then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. It often achieves higher performance than hard voting because it gives more weight to highly confident votes. \n",
        " *  Replace `voting=\"hard\"` with `voting=\"soft\"` and ensure that all your classifiers can estimate class probabilities. \n",
        "  * If you use the SVC estimator, you need to set `probability = True`, which will make the SVC class use cross-validation to estimate class probabilities and add a predict_proba() method. \n",
        "\n",
        "* Part 2: Implement Ensemble Learning on the Boston Dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM8-EZ7S1pNA"
      },
      "source": [
        "## EXERCISE SECTION 7: PART 1 \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHtG2PNJpFFH"
      },
      "source": [
        "## EXERCISE SECTION 7: PART 2 \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojriAzfkgMXC"
      },
      "source": [
        "## 8. Building Pipelines\n",
        "* We can build a pipeline so that many of the steps we have covered to now can be implemented in one step, and be applied to the training and testing set without repeating code.\n",
        "* We will cover one implementation of a pipeline. The exercise below will link to further information for you to be able implement your own pipelines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR_8ZldC1rvI"
      },
      "source": [
        "# import the libraries - these are dependent on what we wish to implement in our pipelines\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# this is the pipeline class required to implement all pipelines\n",
        "from sklearn.pipeline import make_pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEmL5PNwslpH"
      },
      "source": [
        "# here we instantiate the pipeline with our methods\n",
        "pipe_lr = make_pipeline(StandardScaler(),                    # takes an arbitary number of transfomers\n",
        "                        PCA(n_components=2),\n",
        "                        LogisticRegression(random_state=1))  # but ends with an estimator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhAe2-Wdslzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92f7bbf9-403d-4c0b-ee13-c64bd8990f15"
      },
      "source": [
        "# here we fit our pipeline to the data, so that the steps above are executed on the training set\n",
        "pipe_lr.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('standardscaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('pca',\n",
              "                 PCA(copy=True, iterated_power='auto', n_components=2,\n",
              "                     random_state=None, svd_solver='auto', tol=0.0,\n",
              "                     whiten=False)),\n",
              "                ('logisticregression',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=1,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2dROmyLvph8"
      },
      "source": [
        "# here we call the predict method on our pipeline model against the test set \n",
        "y_pred = pipe_lr.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVyLf9QNvKzG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "528aa8e8-0b1f-4fde-a0ac-4c8db56ced07"
      },
      "source": [
        "# and print out the result\n",
        "print('Test Accuracy: %.3f' % accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn3FnXs71r53"
      },
      "source": [
        "### EXERCISE FOR SECTION 8: \n",
        "* Implement a pipeline on the classification dataset (the one used in the example above). Specifically, implement `K-Fold Cross Validation` as set out in  *Python Machine Learning*, Chapter 6.\n",
        "  Moreinformation on pipelines is available at this [link](https://www.kdnuggets.com/2017/12/managing-machine-learning-workflows-scikit-learn-pipelines-part-1.html) blog series \n",
        "* Think about:\n",
        "  * the code being implemented for `K-Fold Cross Validation` as part of the pipeline. Put comments above the code to explain what is happening. \n",
        "  * how you want to preprocess the data\n",
        "  * whether you want to think about reducing the dimensions of the data with something like `Principal Component Analysis` and why\n",
        "  * what algorithm you want to use \n",
        "  * make sure to predict the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJPj73ve1sG8"
      },
      "source": [
        "## EXERCISE CODE HERE\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7jIn9fwgMew"
      },
      "source": [
        "## 9. Review\n",
        "* We have covered:\n",
        "  * Scaling and Standardizing Data \n",
        "  * Model Evaluation and Metrics \n",
        "  * Classification and regression machine learning algorithms  \n",
        "  * Regularization\n",
        "  * Optimising parameters \n",
        "  * Ensemble models \n",
        "  * Building Pipelines\n",
        "* We have of course not covered everything that it is possible to cover, and there is plenty more to research. Please ask questions on Teams or follow-up with your own research."
      ]
    }
  ]
}