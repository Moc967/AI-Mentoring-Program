{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzPHRjwltwtM"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/13W9jpgQCO_tpL9ok5mPRCII9e_wjy6SM#scrollTo=HzPHRjwltwtM.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsTne6XAVbk8"
   },
   "source": [
    "# Regression problem\n",
    "\n",
    "In this project, you will evaluate the performance and predictive power of a model that has been trained and tested on data collected from homes in suburbs of Boston, Massachusetts. A model trained on this data that is seen as a good fit could then be used to make certain predictions about a home â€” in particular, its monetary value. This model would prove to be invaluable for someone like a real estate agent who could make use of such information on a daily basis.\n",
    "\n",
    "The dataset for this project originates from the UCI Machine Learning Repository. The Boston housing data was collected in 1978 and each of the 506 entries represent aggregated data about 14 features for homes from various suburbs in Boston, Massachusetts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xGRIlkVVdhP"
   },
   "source": [
    "We will use the following python libraries, which you will encounter frequently for data analysis and machine learning tasks:\n",
    "\n",
    "- numpy, which provides vectorised arrays, and maths, algebra functionality;\n",
    "- pandas, which provides data structures and data analysis tools;\n",
    "- matplotlib, which provides highly customisable plotting functionality (and we - seaborn, built on top of matplotlib, which is less customisable but can generate charts with less code); and,\n",
    "- scikit-learn, which provides models and tools for most machine learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZBAfuonqPNm"
   },
   "source": [
    "We start by importing necessary packages. sklearn.datasets which is a platform for small standard datasets. SVR which is a Support Vector Regression a subsequent model of SVMs. GridSearchCV to loop through predefined hyperparameters and fit our SVR model on our Boston's training set. Seaborn to statistically analyze our dataset. then respectively import metrics to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WkauN5X-GsOh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.datasets import load_boston #loading the boston house-prices dataset\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqLQy1tzqZHp"
   },
   "source": [
    "Then let's define the functions. \n",
    "- We will start by importing and reorganize the dataset into main columns (features).\n",
    "- Once uploaded, we remove outliers and split the data into training and testing dataset using train_test_split from sklearn.model_selection to shuffle and split the features and prices data. this will actually allow us to use the independent testing dataset to estimate the SVR performance.\n",
    "- Once the dataset is ready, we need to define the model. Keep in mind that SVR is a regression model. To implement it we need to define Regularization parameter and kernel. \n",
    "- In this second step of this project, you will make a cursory investigation about the Boston housing data and provide your observations. Familiarizing yourself with the data through an explorative process is a fundamental practice to help you better understand and justify your results. Since the main goal of this project is to construct a working model which has the capability of predicting the value of houses, we will need to separate the dataset into features and the target variable. The target variable, 'MEDV', will be the variable we seek to predict. These are stored in features and prices, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ap3Qrpcwq59N"
   },
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bY1loZKjGuIX"
   },
   "outputs": [],
   "source": [
    "# we load the dataset and save it as the variable boston\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q82InagGbDe-"
   },
   "outputs": [],
   "source": [
    "# if we want to know what sort of detail is provided with this dataset, we can call .keys()\n",
    "boston.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lm3nvromfKff"
   },
   "outputs": [],
   "source": [
    "# the info at the .DESCR key will tell us more \n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPwYpfTbmhqw"
   },
   "outputs": [],
   "source": [
    "features = boston.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUU7kb1YHeBu"
   },
   "outputs": [],
   "source": [
    "print(f'The features in dataset are: {features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIoyXHHcGyaj"
   },
   "outputs": [],
   "source": [
    "# we can use pandas to create a dataframe, which is basically a way of storing and operating on tabular data \n",
    "# here we pass in both the data and the column names as variables\n",
    "boston_X = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
    "boston_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxvsnMf4gedW"
   },
   "outputs": [],
   "source": [
    "# we can then look at the top of the dataframe to see the sort of values it contains\n",
    "boston_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfuaupmeG35z"
   },
   "outputs": [],
   "source": [
    "# store target values\n",
    "boston_y = boston.target\n",
    "boston_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3knTsqHmqxL"
   },
   "outputs": [],
   "source": [
    "# we can then look at the top of the dataframe to see the sort of values it contains\n",
    "print(f'Data description\\n {boston_X.describe()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7AJ6pjsJZJe"
   },
   "source": [
    "# Plot graphs to understand data\n",
    "\n",
    "Further, We will create in this step a scatterplot matrix that will allow us to visualize the pair-wise relationships and correlations between the different features. It is also quite useful to have a quick overview of how the data is distributed and whether it cointains or not outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6okhuzDIc6o"
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_data(x_df, y_df,features, cor=False):\n",
    "    X = x_df.values\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(\"Price Distribution\")\n",
    "    plt.hist(y_df, bins=30)\n",
    "    plt.show()\n",
    "    #cols = x_df.columns()\n",
    "    fig, ax = plt.subplots(1, len(features), sharey=True, figsize=(20,5))\n",
    "    plt.title(\"Relationship between different input features and price\")\n",
    "    ax = ax.flatten()\n",
    "    for i, col in enumerate(features): #for every feature for all statistics\n",
    "        x = X[:,i]\n",
    "        y = y_df\n",
    "        ax[i].scatter(x, y, marker='o')\n",
    "        ax[i].set_title(col)\n",
    "        ax[i].set_xlabel(col)\n",
    "        ax[i].set_ylabel('MEDV')\n",
    "    plt.show()\n",
    "#Plotting the heatmap of correlation between features, The correlation coefficient \n",
    "#ranges from -1 to 1. If the value is close  to 1, \n",
    "#it means that there is a strong positive correlation between the  two variables.\n",
    " #When it is close to -1, the variables have a strong  negative correlation.\n",
    "    if cor:\n",
    "      corr = x_df.corr()\n",
    "      plt.figure(figsize=(10,10))\n",
    "      sns.heatmap(corr, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap='Greens')\n",
    "      plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6cmJnycIrTb"
   },
   "outputs": [],
   "source": [
    "# call function to display the charts\n",
    "plot_data(boston_X, boston_y, features, cor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFKSj3cXyk3h"
   },
   "source": [
    "A histogram tells is the number of times, or frequency, a value occurs within a bin, or bucket, that splits the data (and which we defined). A histogram shows the frequency with which values occur within each of these bins, and can tell us about the distribution of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRyLoRgzngGV"
   },
   "source": [
    "# Preprocess data\n",
    "\n",
    "In this Boston Dataset we need not to clean the data. The dataset already cleaned when we download it from the sklearn.datasets. By looking at data ploting resulting on the code above, it is obvious that normalizing the data would be a good practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bs9KeoItIuJy"
   },
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "def remove_outliers(x,y, features):\n",
    "    #remove null\n",
    "    x_df = x.copy(deep=True)\n",
    "    x_df['MEDV'] = y\n",
    "    x_df.dropna(inplace=True)\n",
    "    return x_df[features], x_df['MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwwZnVoOJyWc"
   },
   "outputs": [],
   "source": [
    "boston_X, boston_y = remove_outliers(boston_X,boston_y, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kIj8Q4XTMjGN"
   },
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "\n",
    "# we can now define the scaler we want to use and apply it to our dataset \n",
    "\n",
    "# a good exercise would be to research waht StandardScaler does - it is from the scikit learn library \n",
    "\n",
    "def scale_numeric(df):\n",
    "    x = df.values \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    x_scaled = scaler.fit_transform(x)\n",
    "    df = pd.DataFrame(x_scaled)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqFm0lTBMkzh"
   },
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "def preprocess(x, y, features):\n",
    "    x_df = x[features].copy(deep=True)\n",
    "    x_df = scale_numeric(x_df)\n",
    "    #print(len(x_df),len(y))\n",
    "    # Split data into train, test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_df,y, test_size=0.3, random_state=1)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jiuEnDq9kFDr"
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = preprocess(boston_X, boston_y, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ef3wHPauoEYS"
   },
   "source": [
    "# Train the model\n",
    "\n",
    "Once the data processed and ready to use, it is now fed to the model! Now let's have fun !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LInwO8NvoGuw"
   },
   "outputs": [],
   "source": [
    "# Train model \n",
    "def train(model,X_train, y_train):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2u69UUyo-UJ"
   },
   "outputs": [],
   "source": [
    "model = SVR() #LinearRegression()\n",
    "\n",
    "model = train(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WmxsJTzoKA3"
   },
   "source": [
    "#Evaluate the model\n",
    "\n",
    "Once We run the training, we evaluate our model using different metrics predifined in the sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxLptiVCodNS"
   },
   "outputs": [],
   "source": [
    "# Step - 6 - Evaluate Model\n",
    "def evaluate(model, X_test, y_test, plot = True, print_results=True, bl=False):\n",
    "    y_pred = model.predict(X_test)\n",
    "    if print_results:\n",
    "      if bl:\n",
    "        print('\\n\\nBaseline Model Performance on Test Dataset:\\n')\n",
    "      else:\n",
    "        print('\\n\\nBest Model Performance on Test Dataset:\\n')\n",
    "      print('R^2:',metrics.r2_score(y_test, y_pred))\n",
    "      print('MAE:',metrics.mean_absolute_error(y_test, y_pred))\n",
    "      print('MSE:',metrics.mean_squared_error(y_test, y_pred))\n",
    "      print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "     \n",
    "    if plot:\n",
    "      plt.scatter(y_test, y_pred)\n",
    "      plt.xlabel(\"Prices\")\n",
    "      plt.ylabel(\"Predicted prices\")\n",
    "      plt.title(\"Prices vs Predicted prices\")\n",
    "      plt.show()\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFazXIdK_mjN"
   },
   "source": [
    "Common metrics below:\n",
    "- Mean Absolute Error: which provides a mean score for all the predicted versus actual values as an absolute value\n",
    "- Means Squared Error: which provides a mean score for all the predicted versus actual values as a square of the absolute value\n",
    "- R2: which we recommend you research as an exercise to grow your knowledge. WIkipedia and sklearn document are a great place to start!\n",
    "- Root Mean Square Error: is the standard deviation of the residuals (prediction errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0c0E1nzFpBuZ"
   },
   "outputs": [],
   "source": [
    "evaluate(model, X_test, y_test, bl= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ShnkY80oOtb"
   },
   "source": [
    "# Improve the model\n",
    "\n",
    "In the code block below, we will need to implement code so that the fit_model function does the following:\n",
    "\n",
    "Create a scoring function using the same performance metric as in Step 6. See the sklearn make_scorer documentation.\n",
    "Build a GridSearchCV object using regressor, parameters. See the sklearn documentation on GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mh__dT2WoQqg"
   },
   "outputs": [],
   "source": [
    "# Step - 7 - Improve Model\n",
    "def optimize_models(X_train, y_train):\n",
    "  params = {'kernel':['linear', 'rbf'], 'C':[1, 10]}\n",
    "  model = SVR()\n",
    "  clf = GridSearchCV(model, params)\n",
    "  clf.fit(X_train, y_train)\n",
    "  return (clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIySfjsBpFaN"
   },
   "outputs": [],
   "source": [
    "best_params = optimize_models(X_train, y_train)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJf-yEeppH3F"
   },
   "outputs": [],
   "source": [
    "## Build Best Model\n",
    "best_C= best_params['C']\n",
    "best_kernel = best_params['kernel']\n",
    "\n",
    "best_model = SVR(kernel = best_kernel, C= best_C)\n",
    "best_model = train(best_model, X_train, y_train)\n",
    "evaluate (best_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3w9MhqJpQIw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Regression - Boston Dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}